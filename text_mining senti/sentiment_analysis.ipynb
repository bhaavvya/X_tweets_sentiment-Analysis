{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48cd084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6e51f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d75efd09",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mbrown\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('brown')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/brown\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Bhavya/nltk_data'\n    - 'C:\\\\Users\\\\Bhavya\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'C:\\\\Users\\\\Bhavya\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Bhavya\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Bhavya\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mbrown\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('brown')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/brown.zip/brown/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Bhavya/nltk_data'\n    - 'C:\\\\Users\\\\Bhavya\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'C:\\\\Users\\\\Bhavya\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Bhavya\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Bhavya\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mbrown\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mbrown\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('brown')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/brown\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Bhavya/nltk_data'\n    - 'C:\\\\Users\\\\Bhavya\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'C:\\\\Users\\\\Bhavya\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Bhavya\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Bhavya\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "brown.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c16166b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement nltk.brown (from versions: none)\n",
      "ERROR: No matching distribution found for nltk.brown\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nltk.brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd4a0db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n",
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n",
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21d65f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e140102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "447ec3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to E:\\...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b18195b",
   "metadata": {},
   "source": [
    "loading the things are there in nltk book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fde4b7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "87dd1319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9c7374f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\",encoding='unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "38324490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Time of Tweet</th>\n",
       "      <th>Age of User</th>\n",
       "      <th>Country</th>\n",
       "      <th>Population -2020</th>\n",
       "      <th>Land Area (Km²)</th>\n",
       "      <th>Density (P/Km²)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>morning</td>\n",
       "      <td>0-20</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>38928346</td>\n",
       "      <td>652860.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "      <td>noon</td>\n",
       "      <td>21-30</td>\n",
       "      <td>Albania</td>\n",
       "      <td>2877797</td>\n",
       "      <td>27400.0</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>43851044</td>\n",
       "      <td>2381740.0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>morning</td>\n",
       "      <td>46-60</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>77265</td>\n",
       "      <td>470.0</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on the releases we already bought</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Angola</td>\n",
       "      <td>32866272</td>\n",
       "      <td>1246700.0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  \\\n",
       "0  cb774db0d1   \n",
       "1  549e992a42   \n",
       "2  088c60f138   \n",
       "3  9642c003ef   \n",
       "4  358bd9e861   \n",
       "\n",
       "                                                                          text  \\\n",
       "0   I`d have responded, if I were going                                          \n",
       "1   Sooo SAD I will miss you here in San Diego!!!                                \n",
       "2  my boss is bullying me...                                                     \n",
       "3   what interview! leave me alone                                               \n",
       "4   Sons of ****, why couldn`t they put them on the releases we already bought   \n",
       "\n",
       "                         selected_text sentiment Time of Tweet Age of User  \\\n",
       "0  I`d have responded, if I were going  neutral   morning       0-20         \n",
       "1  Sooo SAD                             negative  noon          21-30        \n",
       "2  bullying me                          negative  night         31-45        \n",
       "3  leave me alone                       negative  morning       46-60        \n",
       "4  Sons of ****,                        negative  noon          60-70        \n",
       "\n",
       "       Country  Population -2020  Land Area (Km²)  Density (P/Km²)  \n",
       "0  Afghanistan  38928346          652860.0         60               \n",
       "1  Albania      2877797           27400.0          105              \n",
       "2  Algeria      43851044          2381740.0        18               \n",
       "3  Andorra      77265             470.0            164              \n",
       "4  Angola       32866272          1246700.0        26               "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3c7f2f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     I`d have responded, if I were going                                       \n",
       "1     Sooo SAD I will miss you here in San Diego!!!                             \n",
       "2    my boss is bullying me...                                                  \n",
       "3     what interview! leave me alone                                            \n",
       "4     Sons of ****, why couldn`t they put them on the releases we already bought\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f6dd4187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27481, 10)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33078b28",
   "metadata": {},
   "source": [
    "expanding the display of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c6632c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on the releases we already bought</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  \\\n",
       "0  cb774db0d1   \n",
       "1  549e992a42   \n",
       "2  088c60f138   \n",
       "3  9642c003ef   \n",
       "4  358bd9e861   \n",
       "\n",
       "                                                                          text  \n",
       "0   I`d have responded, if I were going                                         \n",
       "1   Sooo SAD I will miss you here in San Diego!!!                               \n",
       "2  my boss is bullying me...                                                    \n",
       "3   what interview! leave me alone                                              \n",
       "4   Sons of ****, why couldn`t they put them on the releases we already bought  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth',1)\n",
    "data = data[['textID','text']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5a9ad8b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['textID'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0bd48e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908f7735",
   "metadata": {},
   "source": [
    "library that contains punctuations is string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a50bfe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ec512aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bcb78675",
   "metadata": {},
   "source": [
    "function to remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8cab97d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    punctuation_free = \"\".join([i for i in text if i not in string.punctuation])\n",
    "    return punctuation_free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d5238b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>Id have responded if I were going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>my boss is bullying me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>what interview leave me alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on the releases we already bought</td>\n",
       "      <td>Sons of  why couldnt they put them on the releases we already bought</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  \\\n",
       "0  cb774db0d1   \n",
       "1  549e992a42   \n",
       "2  088c60f138   \n",
       "3  9642c003ef   \n",
       "4  358bd9e861   \n",
       "\n",
       "                                                                          text  \\\n",
       "0   I`d have responded, if I were going                                          \n",
       "1   Sooo SAD I will miss you here in San Diego!!!                                \n",
       "2  my boss is bullying me...                                                     \n",
       "3   what interview! leave me alone                                               \n",
       "4   Sons of ****, why couldn`t they put them on the releases we already bought   \n",
       "\n",
       "                                                               clean_txt  \n",
       "0   Id have responded if I were going                                     \n",
       "1   Sooo SAD I will miss you here in San Diego                            \n",
       "2  my boss is bullying me                                                 \n",
       "3   what interview leave me alone                                         \n",
       "4   Sons of  why couldnt they put them on the releases we already bought  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['clean_txt'] = data['text'].apply(lambda x: remove_punctuation(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f17fc419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    if isinstance(text, str):\n",
    "        punctuation_free = \"\".join([i for i in text if i not in string.punctuation])\n",
    "        return punctuation_free\n",
    "    else:\n",
    "        return str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3fd174f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with float values:\n",
      "         textID text\n",
      "314  fdb77c3752  NaN\n"
     ]
    }
   ],
   "source": [
    "float_rows = data[data['text'].apply(lambda x: not isinstance(x, str))]\n",
    "\n",
    "# Display or do further processing with the rows containing float values\n",
    "print(\"Rows with float values:\")\n",
    "print(float_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77913a59",
   "metadata": {},
   "source": [
    "check whether there is NAN value in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9eef71a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>fdb77c3752</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         textID text\n",
       "314  fdb77c3752  NaN"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_rows = data[pd.isna(data['text'])]\n",
    "nan_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c547662",
   "metadata": {},
   "source": [
    "Drp that row which has NAN value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d11895f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset=['text'],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "302c9101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>Id have responded if I were going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>my boss is bullying me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>what interview leave me alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on the releases we already bought</td>\n",
       "      <td>Sons of  why couldnt they put them on the releases we already bought</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  \\\n",
       "0  cb774db0d1   \n",
       "1  549e992a42   \n",
       "2  088c60f138   \n",
       "3  9642c003ef   \n",
       "4  358bd9e861   \n",
       "\n",
       "                                                                          text  \\\n",
       "0   I`d have responded, if I were going                                          \n",
       "1   Sooo SAD I will miss you here in San Diego!!!                                \n",
       "2  my boss is bullying me...                                                     \n",
       "3   what interview! leave me alone                                               \n",
       "4   Sons of ****, why couldn`t they put them on the releases we already bought   \n",
       "\n",
       "                                                               clean_txt  \n",
       "0   Id have responded if I were going                                     \n",
       "1   Sooo SAD I will miss you here in San Diego                            \n",
       "2  my boss is bullying me                                                 \n",
       "3   what interview leave me alone                                         \n",
       "4   Sons of  why couldnt they put them on the releases we already bought  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "52ac01b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_txt</th>\n",
       "      <th>lower_txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>Id have responded if I were going</td>\n",
       "      <td>id have responded if i were going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego</td>\n",
       "      <td>sooo sad i will miss you here in san diego</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>my boss is bullying me</td>\n",
       "      <td>my boss is bullying me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>what interview leave me alone</td>\n",
       "      <td>what interview leave me alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on the releases we already bought</td>\n",
       "      <td>Sons of  why couldnt they put them on the releases we already bought</td>\n",
       "      <td>sons of  why couldnt they put them on the releases we already bought</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  \\\n",
       "0  cb774db0d1   \n",
       "1  549e992a42   \n",
       "2  088c60f138   \n",
       "3  9642c003ef   \n",
       "4  358bd9e861   \n",
       "\n",
       "                                                                          text  \\\n",
       "0   I`d have responded, if I were going                                          \n",
       "1   Sooo SAD I will miss you here in San Diego!!!                                \n",
       "2  my boss is bullying me...                                                     \n",
       "3   what interview! leave me alone                                               \n",
       "4   Sons of ****, why couldn`t they put them on the releases we already bought   \n",
       "\n",
       "                                                               clean_txt  \\\n",
       "0   Id have responded if I were going                                      \n",
       "1   Sooo SAD I will miss you here in San Diego                             \n",
       "2  my boss is bullying me                                                  \n",
       "3   what interview leave me alone                                          \n",
       "4   Sons of  why couldnt they put them on the releases we already bought   \n",
       "\n",
       "                                                               lower_txt  \n",
       "0   id have responded if i were going                                     \n",
       "1   sooo sad i will miss you here in san diego                            \n",
       "2  my boss is bullying me                                                 \n",
       "3   what interview leave me alone                                         \n",
       "4   sons of  why couldnt they put them on the releases we already bought  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['lower_txt'] = data['clean_txt'].apply(lambda x: x.lower())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ed545a0",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bbb8bf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "91e6ee81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(text):\n",
    "    tokens = re.split(' ',text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a3560d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_txt</th>\n",
       "      <th>lower_txt</th>\n",
       "      <th>token_txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>Id have responded if I were going</td>\n",
       "      <td>id have responded if i were going</td>\n",
       "      <td>[, I`d, have, responded,, if, I, were, going]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego</td>\n",
       "      <td>sooo sad i will miss you here in san diego</td>\n",
       "      <td>[, Sooo, SAD, I, will, miss, you, here, in, San, Diego!!!]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>my boss is bullying me</td>\n",
       "      <td>my boss is bullying me</td>\n",
       "      <td>[my, boss, is, bullying, me...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>what interview leave me alone</td>\n",
       "      <td>what interview leave me alone</td>\n",
       "      <td>[, what, interview!, leave, me, alone]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on the releases we already bought</td>\n",
       "      <td>Sons of  why couldnt they put them on the releases we already bought</td>\n",
       "      <td>sons of  why couldnt they put them on the releases we already bought</td>\n",
       "      <td>[, Sons, of, ****,, why, couldn`t, they, put, them, on, the, releases, we, already, bought]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  \\\n",
       "0  cb774db0d1   \n",
       "1  549e992a42   \n",
       "2  088c60f138   \n",
       "3  9642c003ef   \n",
       "4  358bd9e861   \n",
       "\n",
       "                                                                          text  \\\n",
       "0   I`d have responded, if I were going                                          \n",
       "1   Sooo SAD I will miss you here in San Diego!!!                                \n",
       "2  my boss is bullying me...                                                     \n",
       "3   what interview! leave me alone                                               \n",
       "4   Sons of ****, why couldn`t they put them on the releases we already bought   \n",
       "\n",
       "                                                               clean_txt  \\\n",
       "0   Id have responded if I were going                                      \n",
       "1   Sooo SAD I will miss you here in San Diego                             \n",
       "2  my boss is bullying me                                                  \n",
       "3   what interview leave me alone                                          \n",
       "4   Sons of  why couldnt they put them on the releases we already bought   \n",
       "\n",
       "                                                               lower_txt  \\\n",
       "0   id have responded if i were going                                      \n",
       "1   sooo sad i will miss you here in san diego                             \n",
       "2  my boss is bullying me                                                  \n",
       "3   what interview leave me alone                                          \n",
       "4   sons of  why couldnt they put them on the releases we already bought   \n",
       "\n",
       "                                                                                     token_txt  \n",
       "0  [, I`d, have, responded,, if, I, were, going]                                                \n",
       "1  [, Sooo, SAD, I, will, miss, you, here, in, San, Diego!!!]                                   \n",
       "2  [my, boss, is, bullying, me...]                                                              \n",
       "3  [, what, interview!, leave, me, alone]                                                       \n",
       "4  [, Sons, of, ****,, why, couldn`t, they, put, them, on, the, releases, we, already, bought]  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['token_txt'] = data['text'].apply(lambda x: tokenization(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9998e8f5",
   "metadata": {},
   "source": [
    "Stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "20a65d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c47bc65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7fda7c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    output = [i for i in text if i not in stopwords]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "254f88e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1351b424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_txt</th>\n",
       "      <th>lower_txt</th>\n",
       "      <th>token_txt</th>\n",
       "      <th>removed_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>Id have responded if I were going</td>\n",
       "      <td>id have responded if i were going</td>\n",
       "      <td>[, I`d, have, responded,, if, I, were, going]</td>\n",
       "      <td>[, I`d, responded,, I, going]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego</td>\n",
       "      <td>sooo sad i will miss you here in san diego</td>\n",
       "      <td>[, Sooo, SAD, I, will, miss, you, here, in, San, Diego!!!]</td>\n",
       "      <td>[, Sooo, SAD, I, miss, San, Diego!!!]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>my boss is bullying me</td>\n",
       "      <td>my boss is bullying me</td>\n",
       "      <td>[my, boss, is, bullying, me...]</td>\n",
       "      <td>[boss, bullying, me...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>what interview leave me alone</td>\n",
       "      <td>what interview leave me alone</td>\n",
       "      <td>[, what, interview!, leave, me, alone]</td>\n",
       "      <td>[, interview!, leave, alone]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on the releases we already bought</td>\n",
       "      <td>Sons of  why couldnt they put them on the releases we already bought</td>\n",
       "      <td>sons of  why couldnt they put them on the releases we already bought</td>\n",
       "      <td>[, Sons, of, ****,, why, couldn`t, they, put, them, on, the, releases, we, already, bought]</td>\n",
       "      <td>[, Sons, ****,, couldn`t, put, releases, already, bought]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  \\\n",
       "0  cb774db0d1   \n",
       "1  549e992a42   \n",
       "2  088c60f138   \n",
       "3  9642c003ef   \n",
       "4  358bd9e861   \n",
       "\n",
       "                                                                          text  \\\n",
       "0   I`d have responded, if I were going                                          \n",
       "1   Sooo SAD I will miss you here in San Diego!!!                                \n",
       "2  my boss is bullying me...                                                     \n",
       "3   what interview! leave me alone                                               \n",
       "4   Sons of ****, why couldn`t they put them on the releases we already bought   \n",
       "\n",
       "                                                               clean_txt  \\\n",
       "0   Id have responded if I were going                                      \n",
       "1   Sooo SAD I will miss you here in San Diego                             \n",
       "2  my boss is bullying me                                                  \n",
       "3   what interview leave me alone                                          \n",
       "4   Sons of  why couldnt they put them on the releases we already bought   \n",
       "\n",
       "                                                               lower_txt  \\\n",
       "0   id have responded if i were going                                      \n",
       "1   sooo sad i will miss you here in san diego                             \n",
       "2  my boss is bullying me                                                  \n",
       "3   what interview leave me alone                                          \n",
       "4   sons of  why couldnt they put them on the releases we already bought   \n",
       "\n",
       "                                                                                     token_txt  \\\n",
       "0  [, I`d, have, responded,, if, I, were, going]                                                 \n",
       "1  [, Sooo, SAD, I, will, miss, you, here, in, San, Diego!!!]                                    \n",
       "2  [my, boss, is, bullying, me...]                                                               \n",
       "3  [, what, interview!, leave, me, alone]                                                        \n",
       "4  [, Sons, of, ****,, why, couldn`t, they, put, them, on, the, releases, we, already, bought]   \n",
       "\n",
       "                                           removed_stopwords  \n",
       "0  [, I`d, responded,, I, going]                              \n",
       "1  [, Sooo, SAD, I, miss, San, Diego!!!]                      \n",
       "2  [boss, bullying, me...]                                    \n",
       "3  [, interview!, leave, alone]                               \n",
       "4  [, Sons, ****,, couldn`t, put, releases, already, bought]  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['removed_stopwords'] = data['token_txt'].apply(lambda x: remove_stopwords(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beaff82",
   "metadata": {},
   "source": [
    "stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2e4512ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4772fb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "412ca269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_text(text):\n",
    "    stemming_text = [porter_stemmer.stem(word) for word in text]\n",
    "    return stemming_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b0a219ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_txt</th>\n",
       "      <th>lower_txt</th>\n",
       "      <th>token_txt</th>\n",
       "      <th>removed_stopwords</th>\n",
       "      <th>Stem_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>Id have responded if I were going</td>\n",
       "      <td>id have responded if i were going</td>\n",
       "      <td>[, I`d, have, responded,, if, I, were, going]</td>\n",
       "      <td>[, I`d, responded,, I, going]</td>\n",
       "      <td>[, i`d, responded,, i, go]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego</td>\n",
       "      <td>sooo sad i will miss you here in san diego</td>\n",
       "      <td>[, Sooo, SAD, I, will, miss, you, here, in, San, Diego!!!]</td>\n",
       "      <td>[, Sooo, SAD, I, miss, San, Diego!!!]</td>\n",
       "      <td>[, sooo, sad, i, miss, san, diego!!!]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>my boss is bullying me</td>\n",
       "      <td>my boss is bullying me</td>\n",
       "      <td>[my, boss, is, bullying, me...]</td>\n",
       "      <td>[boss, bullying, me...]</td>\n",
       "      <td>[boss, bulli, me...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>what interview leave me alone</td>\n",
       "      <td>what interview leave me alone</td>\n",
       "      <td>[, what, interview!, leave, me, alone]</td>\n",
       "      <td>[, interview!, leave, alone]</td>\n",
       "      <td>[, interview!, leav, alon]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on the releases we already bought</td>\n",
       "      <td>Sons of  why couldnt they put them on the releases we already bought</td>\n",
       "      <td>sons of  why couldnt they put them on the releases we already bought</td>\n",
       "      <td>[, Sons, of, ****,, why, couldn`t, they, put, them, on, the, releases, we, already, bought]</td>\n",
       "      <td>[, Sons, ****,, couldn`t, put, releases, already, bought]</td>\n",
       "      <td>[, son, ****,, couldn`t, put, releas, alreadi, bought]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  \\\n",
       "0  cb774db0d1   \n",
       "1  549e992a42   \n",
       "2  088c60f138   \n",
       "3  9642c003ef   \n",
       "4  358bd9e861   \n",
       "\n",
       "                                                                          text  \\\n",
       "0   I`d have responded, if I were going                                          \n",
       "1   Sooo SAD I will miss you here in San Diego!!!                                \n",
       "2  my boss is bullying me...                                                     \n",
       "3   what interview! leave me alone                                               \n",
       "4   Sons of ****, why couldn`t they put them on the releases we already bought   \n",
       "\n",
       "                                                               clean_txt  \\\n",
       "0   Id have responded if I were going                                      \n",
       "1   Sooo SAD I will miss you here in San Diego                             \n",
       "2  my boss is bullying me                                                  \n",
       "3   what interview leave me alone                                          \n",
       "4   Sons of  why couldnt they put them on the releases we already bought   \n",
       "\n",
       "                                                               lower_txt  \\\n",
       "0   id have responded if i were going                                      \n",
       "1   sooo sad i will miss you here in san diego                             \n",
       "2  my boss is bullying me                                                  \n",
       "3   what interview leave me alone                                          \n",
       "4   sons of  why couldnt they put them on the releases we already bought   \n",
       "\n",
       "                                                                                     token_txt  \\\n",
       "0  [, I`d, have, responded,, if, I, were, going]                                                 \n",
       "1  [, Sooo, SAD, I, will, miss, you, here, in, San, Diego!!!]                                    \n",
       "2  [my, boss, is, bullying, me...]                                                               \n",
       "3  [, what, interview!, leave, me, alone]                                                        \n",
       "4  [, Sons, of, ****,, why, couldn`t, they, put, them, on, the, releases, we, already, bought]   \n",
       "\n",
       "                                           removed_stopwords  \\\n",
       "0  [, I`d, responded,, I, going]                               \n",
       "1  [, Sooo, SAD, I, miss, San, Diego!!!]                       \n",
       "2  [boss, bullying, me...]                                     \n",
       "3  [, interview!, leave, alone]                                \n",
       "4  [, Sons, ****,, couldn`t, put, releases, already, bought]   \n",
       "\n",
       "                                                Stem_text  \n",
       "0  [, i`d, responded,, i, go]                              \n",
       "1  [, sooo, sad, i, miss, san, diego!!!]                   \n",
       "2  [boss, bulli, me...]                                    \n",
       "3  [, interview!, leav, alon]                              \n",
       "4  [, son, ****,, couldn`t, put, releas, alreadi, bought]  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Stem_text'] = data['removed_stopwords'].apply(lambda x: stem_text(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f75bea",
   "metadata": {},
   "source": [
    "Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3156e76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0c8c242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "483fb743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_text(text):\n",
    "    lemma_txt = [word_lemma.lemmatize(word) for word in text]\n",
    "    return lemma_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "00f9fb2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_txt</th>\n",
       "      <th>lower_txt</th>\n",
       "      <th>token_txt</th>\n",
       "      <th>removed_stopwords</th>\n",
       "      <th>Stem_text</th>\n",
       "      <th>lemma_txt</th>\n",
       "      <th>senti_vadar</th>\n",
       "      <th>final_senti</th>\n",
       "      <th>senti</th>\n",
       "      <th>final</th>\n",
       "      <th>final_senti_vadar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>Id have responded if I were going</td>\n",
       "      <td>id have responded if i were going</td>\n",
       "      <td>[, I`d, have, responded,, if, I, were, going]</td>\n",
       "      <td>[, I`d, responded,, I, going]</td>\n",
       "      <td>[, i`d, responded,, i, go]</td>\n",
       "      <td>[, I`d, responded,, I, going]</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego</td>\n",
       "      <td>sooo sad i will miss you here in san diego</td>\n",
       "      <td>[, Sooo, SAD, I, will, miss, you, here, in, San, Diego!!!]</td>\n",
       "      <td>[, Sooo, SAD, I, miss, San, Diego!!!]</td>\n",
       "      <td>[, sooo, sad, i, miss, san, diego!!!]</td>\n",
       "      <td>[, Sooo, SAD, I, miss, San, Diego!!!]</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "      <td>neutral</td>\n",
       "      <td>-0.976562</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>my boss is bullying me</td>\n",
       "      <td>my boss is bullying me</td>\n",
       "      <td>[my, boss, is, bullying, me...]</td>\n",
       "      <td>[boss, bullying, me...]</td>\n",
       "      <td>[boss, bulli, me...]</td>\n",
       "      <td>[bos, bullying, me...]</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>what interview leave me alone</td>\n",
       "      <td>what interview leave me alone</td>\n",
       "      <td>[, what, interview!, leave, me, alone]</td>\n",
       "      <td>[, interview!, leave, alone]</td>\n",
       "      <td>[, interview!, leav, alon]</td>\n",
       "      <td>[, interview!, leave, alone]</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on the releases we already bought</td>\n",
       "      <td>Sons of  why couldnt they put them on the releases we already bought</td>\n",
       "      <td>sons of  why couldnt they put them on the releases we already bought</td>\n",
       "      <td>[, Sons, of, ****,, why, couldn`t, they, put, them, on, the, releases, we, already, bought]</td>\n",
       "      <td>[, Sons, ****,, couldn`t, put, releases, already, bought]</td>\n",
       "      <td>[, son, ****,, couldn`t, put, releas, alreadi, bought]</td>\n",
       "      <td>[, Sons, ****,, couldn`t, put, release, already, bought]</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  \\\n",
       "0  cb774db0d1   \n",
       "1  549e992a42   \n",
       "2  088c60f138   \n",
       "3  9642c003ef   \n",
       "4  358bd9e861   \n",
       "\n",
       "                                                                          text  \\\n",
       "0   I`d have responded, if I were going                                          \n",
       "1   Sooo SAD I will miss you here in San Diego!!!                                \n",
       "2  my boss is bullying me...                                                     \n",
       "3   what interview! leave me alone                                               \n",
       "4   Sons of ****, why couldn`t they put them on the releases we already bought   \n",
       "\n",
       "                                                               clean_txt  \\\n",
       "0   Id have responded if I were going                                      \n",
       "1   Sooo SAD I will miss you here in San Diego                             \n",
       "2  my boss is bullying me                                                  \n",
       "3   what interview leave me alone                                          \n",
       "4   Sons of  why couldnt they put them on the releases we already bought   \n",
       "\n",
       "                                                               lower_txt  \\\n",
       "0   id have responded if i were going                                      \n",
       "1   sooo sad i will miss you here in san diego                             \n",
       "2  my boss is bullying me                                                  \n",
       "3   what interview leave me alone                                          \n",
       "4   sons of  why couldnt they put them on the releases we already bought   \n",
       "\n",
       "                                                                                     token_txt  \\\n",
       "0  [, I`d, have, responded,, if, I, were, going]                                                 \n",
       "1  [, Sooo, SAD, I, will, miss, you, here, in, San, Diego!!!]                                    \n",
       "2  [my, boss, is, bullying, me...]                                                               \n",
       "3  [, what, interview!, leave, me, alone]                                                        \n",
       "4  [, Sons, of, ****,, why, couldn`t, they, put, them, on, the, releases, we, already, bought]   \n",
       "\n",
       "                                           removed_stopwords  \\\n",
       "0  [, I`d, responded,, I, going]                               \n",
       "1  [, Sooo, SAD, I, miss, San, Diego!!!]                       \n",
       "2  [boss, bullying, me...]                                     \n",
       "3  [, interview!, leave, alone]                                \n",
       "4  [, Sons, ****,, couldn`t, put, releases, already, bought]   \n",
       "\n",
       "                                                Stem_text  \\\n",
       "0  [, i`d, responded,, i, go]                               \n",
       "1  [, sooo, sad, i, miss, san, diego!!!]                    \n",
       "2  [boss, bulli, me...]                                     \n",
       "3  [, interview!, leav, alon]                               \n",
       "4  [, son, ****,, couldn`t, put, releas, alreadi, bought]   \n",
       "\n",
       "                                                  lemma_txt  \\\n",
       "0  [, I`d, responded,, I, going]                              \n",
       "1  [, Sooo, SAD, I, miss, San, Diego!!!]                      \n",
       "2  [bos, bullying, me...]                                     \n",
       "3  [, interview!, leave, alone]                               \n",
       "4  [, Sons, ****,, couldn`t, put, release, already, bought]   \n",
       "\n",
       "                                             senti_vadar final_senti  \\\n",
       "0  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  neutral      \n",
       "1  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  neutral      \n",
       "2  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  neutral      \n",
       "3  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  neutral      \n",
       "4  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  neutral      \n",
       "\n",
       "      senti     final final_senti_vadar  \n",
       "0  0.000000  neutral   neutral           \n",
       "1 -0.976562  negative  neutral           \n",
       "2  0.000000  neutral   neutral           \n",
       "3  0.000000  neutral   neutral           \n",
       "4  0.000000  neutral   neutral           "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['lemma_txt'] = data['removed_stopwords'].apply(lambda x: lemma_text(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a3a04e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d84bf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    " pip install -U textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7b521840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_txt</th>\n",
       "      <th>lower_txt</th>\n",
       "      <th>token_txt</th>\n",
       "      <th>removed_stopwords</th>\n",
       "      <th>Stem_text</th>\n",
       "      <th>lemma_txt</th>\n",
       "      <th>senti_vadar</th>\n",
       "      <th>final_senti</th>\n",
       "      <th>senti</th>\n",
       "      <th>final</th>\n",
       "      <th>final_senti_vadar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>Id have responded if I were going</td>\n",
       "      <td>id have responded if i were going</td>\n",
       "      <td>[, I`d, have, responded,, if, I, were, going]</td>\n",
       "      <td>[, I`d, responded,, I, going]</td>\n",
       "      <td>[, i`d, responded,, i, go]</td>\n",
       "      <td>['', 'I`d', 'responded,', 'I', 'going']</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego</td>\n",
       "      <td>sooo sad i will miss you here in san diego</td>\n",
       "      <td>[, Sooo, SAD, I, will, miss, you, here, in, San, Diego!!!]</td>\n",
       "      <td>[, Sooo, SAD, I, miss, San, Diego!!!]</td>\n",
       "      <td>[, sooo, sad, i, miss, san, diego!!!]</td>\n",
       "      <td>['', 'Sooo', 'SAD', 'I', 'miss', 'San', 'Diego!!!']</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "      <td>neutral</td>\n",
       "      <td>-0.976562</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>my boss is bullying me</td>\n",
       "      <td>my boss is bullying me</td>\n",
       "      <td>[my, boss, is, bullying, me...]</td>\n",
       "      <td>[boss, bullying, me...]</td>\n",
       "      <td>[boss, bulli, me...]</td>\n",
       "      <td>['bos', 'bullying', 'me...']</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>what interview leave me alone</td>\n",
       "      <td>what interview leave me alone</td>\n",
       "      <td>[, what, interview!, leave, me, alone]</td>\n",
       "      <td>[, interview!, leave, alone]</td>\n",
       "      <td>[, interview!, leav, alon]</td>\n",
       "      <td>['', 'interview!', 'leave', 'alone']</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on the releases we already bought</td>\n",
       "      <td>Sons of  why couldnt they put them on the releases we already bought</td>\n",
       "      <td>sons of  why couldnt they put them on the releases we already bought</td>\n",
       "      <td>[, Sons, of, ****,, why, couldn`t, they, put, them, on, the, releases, we, already, bought]</td>\n",
       "      <td>[, Sons, ****,, couldn`t, put, releases, already, bought]</td>\n",
       "      <td>[, son, ****,, couldn`t, put, releas, alreadi, bought]</td>\n",
       "      <td>['', 'Sons', '****,', 'couldn`t', 'put', 'release', 'already', 'bought']</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  \\\n",
       "0  cb774db0d1   \n",
       "1  549e992a42   \n",
       "2  088c60f138   \n",
       "3  9642c003ef   \n",
       "4  358bd9e861   \n",
       "\n",
       "                                                                          text  \\\n",
       "0   I`d have responded, if I were going                                          \n",
       "1   Sooo SAD I will miss you here in San Diego!!!                                \n",
       "2  my boss is bullying me...                                                     \n",
       "3   what interview! leave me alone                                               \n",
       "4   Sons of ****, why couldn`t they put them on the releases we already bought   \n",
       "\n",
       "                                                               clean_txt  \\\n",
       "0   Id have responded if I were going                                      \n",
       "1   Sooo SAD I will miss you here in San Diego                             \n",
       "2  my boss is bullying me                                                  \n",
       "3   what interview leave me alone                                          \n",
       "4   Sons of  why couldnt they put them on the releases we already bought   \n",
       "\n",
       "                                                               lower_txt  \\\n",
       "0   id have responded if i were going                                      \n",
       "1   sooo sad i will miss you here in san diego                             \n",
       "2  my boss is bullying me                                                  \n",
       "3   what interview leave me alone                                          \n",
       "4   sons of  why couldnt they put them on the releases we already bought   \n",
       "\n",
       "                                                                                     token_txt  \\\n",
       "0  [, I`d, have, responded,, if, I, were, going]                                                 \n",
       "1  [, Sooo, SAD, I, will, miss, you, here, in, San, Diego!!!]                                    \n",
       "2  [my, boss, is, bullying, me...]                                                               \n",
       "3  [, what, interview!, leave, me, alone]                                                        \n",
       "4  [, Sons, of, ****,, why, couldn`t, they, put, them, on, the, releases, we, already, bought]   \n",
       "\n",
       "                                           removed_stopwords  \\\n",
       "0  [, I`d, responded,, I, going]                               \n",
       "1  [, Sooo, SAD, I, miss, San, Diego!!!]                       \n",
       "2  [boss, bullying, me...]                                     \n",
       "3  [, interview!, leave, alone]                                \n",
       "4  [, Sons, ****,, couldn`t, put, releases, already, bought]   \n",
       "\n",
       "                                                Stem_text  \\\n",
       "0  [, i`d, responded,, i, go]                               \n",
       "1  [, sooo, sad, i, miss, san, diego!!!]                    \n",
       "2  [boss, bulli, me...]                                     \n",
       "3  [, interview!, leav, alon]                               \n",
       "4  [, son, ****,, couldn`t, put, releas, alreadi, bought]   \n",
       "\n",
       "                                                                  lemma_txt  \\\n",
       "0  ['', 'I`d', 'responded,', 'I', 'going']                                    \n",
       "1  ['', 'Sooo', 'SAD', 'I', 'miss', 'San', 'Diego!!!']                        \n",
       "2  ['bos', 'bullying', 'me...']                                               \n",
       "3  ['', 'interview!', 'leave', 'alone']                                       \n",
       "4  ['', 'Sons', '****,', 'couldn`t', 'put', 'release', 'already', 'bought']   \n",
       "\n",
       "                                             senti_vadar final_senti  \\\n",
       "0  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  neutral      \n",
       "1  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  neutral      \n",
       "2  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  neutral      \n",
       "3  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  neutral      \n",
       "4  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  neutral      \n",
       "\n",
       "      senti     final final_senti_vadar  \n",
       "0  0.000000  neutral   neutral           \n",
       "1 -0.976562  negative  neutral           \n",
       "2  0.000000  neutral   neutral           \n",
       "3  0.000000  neutral   neutral           \n",
       "4  0.000000  neutral   neutral           "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['senti'] = data['text'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "56cc9259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def values(num):\n",
    "    if num < 0:\n",
    "        return \"negative\"\n",
    "    elif num > 0:\n",
    "        return \"positive\"\n",
    "    else:\n",
    "        return \"neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "26f0ea0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_txt</th>\n",
       "      <th>lower_txt</th>\n",
       "      <th>token_txt</th>\n",
       "      <th>removed_stopwords</th>\n",
       "      <th>Stem_text</th>\n",
       "      <th>lemma_txt</th>\n",
       "      <th>senti_vadar</th>\n",
       "      <th>final_senti</th>\n",
       "      <th>senti</th>\n",
       "      <th>final</th>\n",
       "      <th>final_senti_vadar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>Id have responded if I were going</td>\n",
       "      <td>id have responded if i were going</td>\n",
       "      <td>[, I`d, have, responded,, if, I, were, going]</td>\n",
       "      <td>[, I`d, responded,, I, going]</td>\n",
       "      <td>[, i`d, responded,, i, go]</td>\n",
       "      <td>['', 'I`d', 'responded,', 'I', 'going']</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego</td>\n",
       "      <td>sooo sad i will miss you here in san diego</td>\n",
       "      <td>[, Sooo, SAD, I, will, miss, you, here, in, San, Diego!!!]</td>\n",
       "      <td>[, Sooo, SAD, I, miss, San, Diego!!!]</td>\n",
       "      <td>[, sooo, sad, i, miss, san, diego!!!]</td>\n",
       "      <td>['', 'Sooo', 'SAD', 'I', 'miss', 'San', 'Diego!!!']</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "      <td>neutral</td>\n",
       "      <td>-0.976562</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>my boss is bullying me</td>\n",
       "      <td>my boss is bullying me</td>\n",
       "      <td>[my, boss, is, bullying, me...]</td>\n",
       "      <td>[boss, bullying, me...]</td>\n",
       "      <td>[boss, bulli, me...]</td>\n",
       "      <td>['bos', 'bullying', 'me...']</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>what interview leave me alone</td>\n",
       "      <td>what interview leave me alone</td>\n",
       "      <td>[, what, interview!, leave, me, alone]</td>\n",
       "      <td>[, interview!, leave, alone]</td>\n",
       "      <td>[, interview!, leav, alon]</td>\n",
       "      <td>['', 'interview!', 'leave', 'alone']</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on the releases we already bought</td>\n",
       "      <td>Sons of  why couldnt they put them on the releases we already bought</td>\n",
       "      <td>sons of  why couldnt they put them on the releases we already bought</td>\n",
       "      <td>[, Sons, of, ****,, why, couldn`t, they, put, them, on, the, releases, we, already, bought]</td>\n",
       "      <td>[, Sons, ****,, couldn`t, put, releases, already, bought]</td>\n",
       "      <td>[, son, ****,, couldn`t, put, releas, alreadi, bought]</td>\n",
       "      <td>['', 'Sons', '****,', 'couldn`t', 'put', 'release', 'already', 'bought']</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  \\\n",
       "0  cb774db0d1   \n",
       "1  549e992a42   \n",
       "2  088c60f138   \n",
       "3  9642c003ef   \n",
       "4  358bd9e861   \n",
       "\n",
       "                                                                          text  \\\n",
       "0   I`d have responded, if I were going                                          \n",
       "1   Sooo SAD I will miss you here in San Diego!!!                                \n",
       "2  my boss is bullying me...                                                     \n",
       "3   what interview! leave me alone                                               \n",
       "4   Sons of ****, why couldn`t they put them on the releases we already bought   \n",
       "\n",
       "                                                               clean_txt  \\\n",
       "0   Id have responded if I were going                                      \n",
       "1   Sooo SAD I will miss you here in San Diego                             \n",
       "2  my boss is bullying me                                                  \n",
       "3   what interview leave me alone                                          \n",
       "4   Sons of  why couldnt they put them on the releases we already bought   \n",
       "\n",
       "                                                               lower_txt  \\\n",
       "0   id have responded if i were going                                      \n",
       "1   sooo sad i will miss you here in san diego                             \n",
       "2  my boss is bullying me                                                  \n",
       "3   what interview leave me alone                                          \n",
       "4   sons of  why couldnt they put them on the releases we already bought   \n",
       "\n",
       "                                                                                     token_txt  \\\n",
       "0  [, I`d, have, responded,, if, I, were, going]                                                 \n",
       "1  [, Sooo, SAD, I, will, miss, you, here, in, San, Diego!!!]                                    \n",
       "2  [my, boss, is, bullying, me...]                                                               \n",
       "3  [, what, interview!, leave, me, alone]                                                        \n",
       "4  [, Sons, of, ****,, why, couldn`t, they, put, them, on, the, releases, we, already, bought]   \n",
       "\n",
       "                                           removed_stopwords  \\\n",
       "0  [, I`d, responded,, I, going]                               \n",
       "1  [, Sooo, SAD, I, miss, San, Diego!!!]                       \n",
       "2  [boss, bullying, me...]                                     \n",
       "3  [, interview!, leave, alone]                                \n",
       "4  [, Sons, ****,, couldn`t, put, releases, already, bought]   \n",
       "\n",
       "                                                Stem_text  \\\n",
       "0  [, i`d, responded,, i, go]                               \n",
       "1  [, sooo, sad, i, miss, san, diego!!!]                    \n",
       "2  [boss, bulli, me...]                                     \n",
       "3  [, interview!, leav, alon]                               \n",
       "4  [, son, ****,, couldn`t, put, releas, alreadi, bought]   \n",
       "\n",
       "                                                                  lemma_txt  \\\n",
       "0  ['', 'I`d', 'responded,', 'I', 'going']                                    \n",
       "1  ['', 'Sooo', 'SAD', 'I', 'miss', 'San', 'Diego!!!']                        \n",
       "2  ['bos', 'bullying', 'me...']                                               \n",
       "3  ['', 'interview!', 'leave', 'alone']                                       \n",
       "4  ['', 'Sons', '****,', 'couldn`t', 'put', 'release', 'already', 'bought']   \n",
       "\n",
       "                                             senti_vadar final_senti  \\\n",
       "0  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  neutral      \n",
       "1  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  neutral      \n",
       "2  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  neutral      \n",
       "3  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  neutral      \n",
       "4  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  neutral      \n",
       "\n",
       "      senti     final final_senti_vadar  \n",
       "0  0.000000  neutral   neutral           \n",
       "1 -0.976562  negative  neutral           \n",
       "2  0.000000  neutral   neutral           \n",
       "3  0.000000  neutral   neutral           \n",
       "4  0.000000  neutral   neutral           "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['final'] = data['senti'].apply(lambda x: values(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c05148d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['final'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5cf9eb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9a459f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\bhavya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.36.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\bhavya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\bhavya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\bhavya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.23.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bhavya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\bhavya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\bhavya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in c:\\users\\bhavya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\bhavya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\bhavya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\bhavya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\bhavya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\bhavya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\bhavya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\bhavya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bhavya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bhavya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bhavya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bhavya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2023.5.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "32570f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "187e6d4f3bd4453aad454224be30563e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/953 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c19599761f4650bd876f0a87133fa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/669M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f44d4b8a4f17493c903eb8dfe69e6dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/39.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05560fb1682a4ef0b8771d2e2bc81815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/872k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b397f9d125f4d75bb7497d362582681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "\n",
    "\n",
    "# Create a BERT sentiment analysis pipeline\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\", model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "01bfa666",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [116]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Apply the BERT model to each text in the 'text' column and store the results in a new 'senti' column\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msenti_bert\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlemma_txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msentiment_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Display the DataFrame with the new 'senti' column\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemma_txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msenti_bert\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:4764\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4630\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4631\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4636\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4637\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4638\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4639\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4640\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4755\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4756\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4758\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4762\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4764\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:1209\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:1289\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1287\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1288\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1289\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1291\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1295\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1818\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2926\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Input \u001b[1;32mIn [116]\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Apply the BERT model to each text in the 'text' column and store the results in a new 'senti' column\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msenti_bert\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemma_txt\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43msentiment_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Display the DataFrame with the new 'senti' column\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemma_txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msenti_bert\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:156\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m    Classify the text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03m        If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     _legacy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py:1121\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[0;32m   1118\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   1119\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[0;32m   1120\u001b[0m     )\n\u001b[1;32m-> 1121\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m    124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[1;32m--> 125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py:1046\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1044\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1045\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1046\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1047\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:187\u001b[0m, in \u001b[0;36mTextClassificationPipeline._forward\u001b[1;34m(self, model_inputs)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(model_forward)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    186\u001b[0m     model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1564\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1560\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1561\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1562\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1564\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1572\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1574\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1576\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1578\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m   1007\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1008\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1011\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1012\u001b[0m )\n\u001b[1;32m-> 1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1026\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    598\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    604\u001b[0m         output_attentions,\n\u001b[0;32m    605\u001b[0m     )\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    487\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    496\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    419\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    425\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    426\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 427\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    436\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    437\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:308\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    306\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([past_key_value[\u001b[38;5;241m1\u001b[39m], value_layer], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 308\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    309\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(hidden_states))\n\u001b[0;32m    311\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Apply the BERT model to each text in the 'text' column and store the results in a new 'senti' column\n",
    "data['senti_bert'] = data['lemma_txt'].apply(lambda x: sentiment_analysis(x)[0]['label'])\n",
    "\n",
    "# Display the DataFrame with the new 'senti' column\n",
    "print(data[['lemma_txt', 'senti_bert']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "24d763ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "bd8383fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "data['lemma_txt'] = data['lemma_txt'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "32cc7782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_txt</th>\n",
       "      <th>lower_txt</th>\n",
       "      <th>token_txt</th>\n",
       "      <th>removed_stopwords</th>\n",
       "      <th>Stem_text</th>\n",
       "      <th>lemma_txt</th>\n",
       "      <th>senti_vadar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>Id have responded if I were going</td>\n",
       "      <td>id have responded if i were going</td>\n",
       "      <td>[, I`d, have, responded,, if, I, were, going]</td>\n",
       "      <td>[, I`d, responded,, I, going]</td>\n",
       "      <td>[, i`d, responded,, i, go]</td>\n",
       "      <td>['', 'I`d', 'responded,', 'I', 'going']</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego</td>\n",
       "      <td>sooo sad i will miss you here in san diego</td>\n",
       "      <td>[, Sooo, SAD, I, will, miss, you, here, in, San, Diego!!!]</td>\n",
       "      <td>[, Sooo, SAD, I, miss, San, Diego!!!]</td>\n",
       "      <td>[, sooo, sad, i, miss, san, diego!!!]</td>\n",
       "      <td>['', 'Sooo', 'SAD', 'I', 'miss', 'San', 'Diego!!!']</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>my boss is bullying me</td>\n",
       "      <td>my boss is bullying me</td>\n",
       "      <td>[my, boss, is, bullying, me...]</td>\n",
       "      <td>[boss, bullying, me...]</td>\n",
       "      <td>[boss, bulli, me...]</td>\n",
       "      <td>['bos', 'bullying', 'me...']</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>what interview leave me alone</td>\n",
       "      <td>what interview leave me alone</td>\n",
       "      <td>[, what, interview!, leave, me, alone]</td>\n",
       "      <td>[, interview!, leave, alone]</td>\n",
       "      <td>[, interview!, leav, alon]</td>\n",
       "      <td>['', 'interview!', 'leave', 'alone']</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on the releases we already bought</td>\n",
       "      <td>Sons of  why couldnt they put them on the releases we already bought</td>\n",
       "      <td>sons of  why couldnt they put them on the releases we already bought</td>\n",
       "      <td>[, Sons, of, ****,, why, couldn`t, they, put, them, on, the, releases, we, already, bought]</td>\n",
       "      <td>[, Sons, ****,, couldn`t, put, releases, already, bought]</td>\n",
       "      <td>[, son, ****,, couldn`t, put, releas, alreadi, bought]</td>\n",
       "      <td>['', 'Sons', '****,', 'couldn`t', 'put', 'release', 'already', 'bought']</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  \\\n",
       "0  cb774db0d1   \n",
       "1  549e992a42   \n",
       "2  088c60f138   \n",
       "3  9642c003ef   \n",
       "4  358bd9e861   \n",
       "\n",
       "                                                                          text  \\\n",
       "0   I`d have responded, if I were going                                          \n",
       "1   Sooo SAD I will miss you here in San Diego!!!                                \n",
       "2  my boss is bullying me...                                                     \n",
       "3   what interview! leave me alone                                               \n",
       "4   Sons of ****, why couldn`t they put them on the releases we already bought   \n",
       "\n",
       "                                                               clean_txt  \\\n",
       "0   Id have responded if I were going                                      \n",
       "1   Sooo SAD I will miss you here in San Diego                             \n",
       "2  my boss is bullying me                                                  \n",
       "3   what interview leave me alone                                          \n",
       "4   Sons of  why couldnt they put them on the releases we already bought   \n",
       "\n",
       "                                                               lower_txt  \\\n",
       "0   id have responded if i were going                                      \n",
       "1   sooo sad i will miss you here in san diego                             \n",
       "2  my boss is bullying me                                                  \n",
       "3   what interview leave me alone                                          \n",
       "4   sons of  why couldnt they put them on the releases we already bought   \n",
       "\n",
       "                                                                                     token_txt  \\\n",
       "0  [, I`d, have, responded,, if, I, were, going]                                                 \n",
       "1  [, Sooo, SAD, I, will, miss, you, here, in, San, Diego!!!]                                    \n",
       "2  [my, boss, is, bullying, me...]                                                               \n",
       "3  [, what, interview!, leave, me, alone]                                                        \n",
       "4  [, Sons, of, ****,, why, couldn`t, they, put, them, on, the, releases, we, already, bought]   \n",
       "\n",
       "                                           removed_stopwords  \\\n",
       "0  [, I`d, responded,, I, going]                               \n",
       "1  [, Sooo, SAD, I, miss, San, Diego!!!]                       \n",
       "2  [boss, bullying, me...]                                     \n",
       "3  [, interview!, leave, alone]                                \n",
       "4  [, Sons, ****,, couldn`t, put, releases, already, bought]   \n",
       "\n",
       "                                                Stem_text  \\\n",
       "0  [, i`d, responded,, i, go]                               \n",
       "1  [, sooo, sad, i, miss, san, diego!!!]                    \n",
       "2  [boss, bulli, me...]                                     \n",
       "3  [, interview!, leav, alon]                               \n",
       "4  [, son, ****,, couldn`t, put, releas, alreadi, bought]   \n",
       "\n",
       "                                                                  lemma_txt  \\\n",
       "0  ['', 'I`d', 'responded,', 'I', 'going']                                    \n",
       "1  ['', 'Sooo', 'SAD', 'I', 'miss', 'San', 'Diego!!!']                        \n",
       "2  ['bos', 'bullying', 'me...']                                               \n",
       "3  ['', 'interview!', 'leave', 'alone']                                       \n",
       "4  ['', 'Sons', '****,', 'couldn`t', 'put', 'release', 'already', 'bought']   \n",
       "\n",
       "                                             senti_vadar  \n",
       "0  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  \n",
       "1  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  \n",
       "2  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  \n",
       "3  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  \n",
       "4  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['senti_vadar'] = data['lemma_txt'].apply(lambda x: sid.polarity_scores(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a1be6c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>senti_vadar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on the releases we already bought</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27476</th>\n",
       "      <td>4eac33d1c0</td>\n",
       "      <td>wish we could come see u on Denver  husband lost his job and can`t afford it</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27477</th>\n",
       "      <td>4f4c4fc327</td>\n",
       "      <td>I`ve wondered about rake to.  The client has made it clear .NET only, don`t force devs to learn a new lang  #agile #ccnet</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27478</th>\n",
       "      <td>f67aae2310</td>\n",
       "      <td>Yay good for both of you. Enjoy the break - you probably need it after such hectic weekend  Take care hun xxxx</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27479</th>\n",
       "      <td>ed167662a5</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27480</th>\n",
       "      <td>6f7127d9d7</td>\n",
       "      <td>All this flirting going on - The ATG smiles. Yay.  ((hugs))</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27480 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID  \\\n",
       "0      cb774db0d1   \n",
       "1      549e992a42   \n",
       "2      088c60f138   \n",
       "3      9642c003ef   \n",
       "4      358bd9e861   \n",
       "...           ...   \n",
       "27476  4eac33d1c0   \n",
       "27477  4f4c4fc327   \n",
       "27478  f67aae2310   \n",
       "27479  ed167662a5   \n",
       "27480  6f7127d9d7   \n",
       "\n",
       "                                                                                                                             text  \\\n",
       "0       I`d have responded, if I were going                                                                                         \n",
       "1       Sooo SAD I will miss you here in San Diego!!!                                                                               \n",
       "2      my boss is bullying me...                                                                                                    \n",
       "3       what interview! leave me alone                                                                                              \n",
       "4       Sons of ****, why couldn`t they put them on the releases we already bought                                                  \n",
       "...                                                                            ...                                                  \n",
       "27476   wish we could come see u on Denver  husband lost his job and can`t afford it                                                \n",
       "27477   I`ve wondered about rake to.  The client has made it clear .NET only, don`t force devs to learn a new lang  #agile #ccnet   \n",
       "27478   Yay good for both of you. Enjoy the break - you probably need it after such hectic weekend  Take care hun xxxx              \n",
       "27479   But it was worth it  ****.                                                                                                  \n",
       "27480     All this flirting going on - The ATG smiles. Yay.  ((hugs))                                                               \n",
       "\n",
       "                                                 senti_vadar  \n",
       "0      {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  \n",
       "1      {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  \n",
       "2      {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  \n",
       "3      {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  \n",
       "4      {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  \n",
       "...                                                      ...  \n",
       "27476  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  \n",
       "27477  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  \n",
       "27478  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  \n",
       "27479  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  \n",
       "27480  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  \n",
       "\n",
       "[27480 rows x 3 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth',1)\n",
    "data[['textID','text','senti_vadar']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ef045de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "senti_vadar\n",
       "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}    27480\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['senti_vadar'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0d2817fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_label(sentiment_scores):\n",
    "    if sentiment_scores['neu'] == 1.0:\n",
    "        return 'neutral'\n",
    "    elif sentiment_scores['compound'] < 0.0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "cef587b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_txt</th>\n",
       "      <th>lower_txt</th>\n",
       "      <th>token_txt</th>\n",
       "      <th>removed_stopwords</th>\n",
       "      <th>Stem_text</th>\n",
       "      <th>lemma_txt</th>\n",
       "      <th>senti_vadar</th>\n",
       "      <th>final_senti</th>\n",
       "      <th>senti</th>\n",
       "      <th>final</th>\n",
       "      <th>final_senti_vadar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>Id have responded if I were going</td>\n",
       "      <td>id have responded if i were going</td>\n",
       "      <td>[, I`d, have, responded,, if, I, were, going]</td>\n",
       "      <td>[, I`d, responded,, I, going]</td>\n",
       "      <td>[, i`d, responded,, i, go]</td>\n",
       "      <td>['', 'I`d', 'responded,', 'I', 'going']</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego</td>\n",
       "      <td>sooo sad i will miss you here in san diego</td>\n",
       "      <td>[, Sooo, SAD, I, will, miss, you, here, in, San, Diego!!!]</td>\n",
       "      <td>[, Sooo, SAD, I, miss, San, Diego!!!]</td>\n",
       "      <td>[, sooo, sad, i, miss, san, diego!!!]</td>\n",
       "      <td>['', 'Sooo', 'SAD', 'I', 'miss', 'San', 'Diego!!!']</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "      <td>neutral</td>\n",
       "      <td>-0.976562</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>my boss is bullying me</td>\n",
       "      <td>my boss is bullying me</td>\n",
       "      <td>[my, boss, is, bullying, me...]</td>\n",
       "      <td>[boss, bullying, me...]</td>\n",
       "      <td>[boss, bulli, me...]</td>\n",
       "      <td>['bos', 'bullying', 'me...']</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>what interview leave me alone</td>\n",
       "      <td>what interview leave me alone</td>\n",
       "      <td>[, what, interview!, leave, me, alone]</td>\n",
       "      <td>[, interview!, leave, alone]</td>\n",
       "      <td>[, interview!, leav, alon]</td>\n",
       "      <td>['', 'interview!', 'leave', 'alone']</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on the releases we already bought</td>\n",
       "      <td>Sons of  why couldnt they put them on the releases we already bought</td>\n",
       "      <td>sons of  why couldnt they put them on the releases we already bought</td>\n",
       "      <td>[, Sons, of, ****,, why, couldn`t, they, put, them, on, the, releases, we, already, bought]</td>\n",
       "      <td>[, Sons, ****,, couldn`t, put, releases, already, bought]</td>\n",
       "      <td>[, son, ****,, couldn`t, put, releas, alreadi, bought]</td>\n",
       "      <td>['', 'Sons', '****,', 'couldn`t', 'put', 'release', 'already', 'bought']</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  \\\n",
       "0  cb774db0d1   \n",
       "1  549e992a42   \n",
       "2  088c60f138   \n",
       "3  9642c003ef   \n",
       "4  358bd9e861   \n",
       "\n",
       "                                                                          text  \\\n",
       "0   I`d have responded, if I were going                                          \n",
       "1   Sooo SAD I will miss you here in San Diego!!!                                \n",
       "2  my boss is bullying me...                                                     \n",
       "3   what interview! leave me alone                                               \n",
       "4   Sons of ****, why couldn`t they put them on the releases we already bought   \n",
       "\n",
       "                                                               clean_txt  \\\n",
       "0   Id have responded if I were going                                      \n",
       "1   Sooo SAD I will miss you here in San Diego                             \n",
       "2  my boss is bullying me                                                  \n",
       "3   what interview leave me alone                                          \n",
       "4   Sons of  why couldnt they put them on the releases we already bought   \n",
       "\n",
       "                                                               lower_txt  \\\n",
       "0   id have responded if i were going                                      \n",
       "1   sooo sad i will miss you here in san diego                             \n",
       "2  my boss is bullying me                                                  \n",
       "3   what interview leave me alone                                          \n",
       "4   sons of  why couldnt they put them on the releases we already bought   \n",
       "\n",
       "                                                                                     token_txt  \\\n",
       "0  [, I`d, have, responded,, if, I, were, going]                                                 \n",
       "1  [, Sooo, SAD, I, will, miss, you, here, in, San, Diego!!!]                                    \n",
       "2  [my, boss, is, bullying, me...]                                                               \n",
       "3  [, what, interview!, leave, me, alone]                                                        \n",
       "4  [, Sons, of, ****,, why, couldn`t, they, put, them, on, the, releases, we, already, bought]   \n",
       "\n",
       "                                           removed_stopwords  \\\n",
       "0  [, I`d, responded,, I, going]                               \n",
       "1  [, Sooo, SAD, I, miss, San, Diego!!!]                       \n",
       "2  [boss, bullying, me...]                                     \n",
       "3  [, interview!, leave, alone]                                \n",
       "4  [, Sons, ****,, couldn`t, put, releases, already, bought]   \n",
       "\n",
       "                                                Stem_text  \\\n",
       "0  [, i`d, responded,, i, go]                               \n",
       "1  [, sooo, sad, i, miss, san, diego!!!]                    \n",
       "2  [boss, bulli, me...]                                     \n",
       "3  [, interview!, leav, alon]                               \n",
       "4  [, son, ****,, couldn`t, put, releas, alreadi, bought]   \n",
       "\n",
       "                                                                  lemma_txt  \\\n",
       "0  ['', 'I`d', 'responded,', 'I', 'going']                                    \n",
       "1  ['', 'Sooo', 'SAD', 'I', 'miss', 'San', 'Diego!!!']                        \n",
       "2  ['bos', 'bullying', 'me...']                                               \n",
       "3  ['', 'interview!', 'leave', 'alone']                                       \n",
       "4  ['', 'Sons', '****,', 'couldn`t', 'put', 'release', 'already', 'bought']   \n",
       "\n",
       "                                             senti_vadar final_senti  \\\n",
       "0  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  neutral      \n",
       "1  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  neutral      \n",
       "2  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  neutral      \n",
       "3  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  neutral      \n",
       "4  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  neutral      \n",
       "\n",
       "      senti     final final_senti_vadar  \n",
       "0  0.000000  neutral   neutral           \n",
       "1 -0.976562  negative  neutral           \n",
       "2  0.000000  neutral   neutral           \n",
       "3  0.000000  neutral   neutral           \n",
       "4  0.000000  neutral   neutral           "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['final_senti_vadar'] = data['senti_vadar'].apply(determine_label)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3f9e02a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "final_senti_vadar\n",
       "neutral    27480\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['final_senti_vadar'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b57cf66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7896d439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Obtaining dependency information for seaborn from https://files.pythonhosted.org/packages/7b/e5/83fcd7e9db036c179e0352bfcd20f81d728197a16f883e7b90307a88e65e/seaborn-0.13.0-py3-none-any.whl.metadata\n",
      "  Downloading seaborn-0.13.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\bhavya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from seaborn) (1.23.0)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\bhavya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from seaborn) (2.1.4)\n",
      "Collecting matplotlib!=3.6.1,>=3.3 (from seaborn)\n",
      "  Obtaining dependency information for matplotlib!=3.6.1,>=3.3 from https://files.pythonhosted.org/packages/46/37/b5e27ab30ecc0a3694c8a78287b5ef35dad0c3095c144fcc43081170bfd6/matplotlib-3.8.2-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading matplotlib-3.8.2-cp310-cp310-win_amd64.whl.metadata (5.9 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib!=3.6.1,>=3.3->seaborn)\n",
      "  Obtaining dependency information for contourpy>=1.0.1 from https://files.pythonhosted.org/packages/fd/7c/168f8343f33d861305e18c56901ef1bb675d3c7f977f435ec72751a71a54/contourpy-1.2.0-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading contourpy-1.2.0-cp310-cp310-win_amd64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib!=3.6.1,>=3.3->seaborn)\n",
      "  Obtaining dependency information for cycler>=0.10 from https://files.pythonhosted.org/packages/e7/05/c19819d5e3d95294a6f5947fb9b9629efb316b96de511b418c53d245aae6/cycler-0.12.1-py3-none-any.whl.metadata\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib!=3.6.1,>=3.3->seaborn)\n",
      "  Obtaining dependency information for fonttools>=4.22.0 from https://files.pythonhosted.org/packages/3c/35/f37c8bda77eee9d4d3a7306ad1d2d71d1e8739fd4f574fac07bd63ddc293/fonttools-4.46.0-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading fonttools-4.46.0-cp310-cp310-win_amd64.whl.metadata (159 kB)\n",
      "     -------------------------------------- 159.4/159.4 kB 2.4 MB/s eta 0:00:00\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib!=3.6.1,>=3.3->seaborn)\n",
      "  Obtaining dependency information for kiwisolver>=1.3.1 from https://files.pythonhosted.org/packages/4a/a1/8a9c9be45c642fa12954855d8b3a02d9fd8551165a558835a19508fec2e6/kiwisolver-1.4.5-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading kiwisolver-1.4.5-cp310-cp310-win_amd64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bhavya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (21.3)\n",
      "Collecting pillow>=8 (from matplotlib!=3.6.1,>=3.3->seaborn)\n",
      "  Obtaining dependency information for pillow>=8 from https://files.pythonhosted.org/packages/2d/7e/18ffce67b6e7637eead295b8a78d293d170d404a633010c3549da9a5e674/Pillow-10.1.0-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading Pillow-10.1.0-cp310-cp310-win_amd64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\bhavya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\bhavya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\bhavya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.2->seaborn) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\bhavya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\bhavya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.3->seaborn) (1.16.0)\n",
      "Downloading seaborn-0.13.0-py3-none-any.whl (294 kB)\n",
      "   ---------------------------------------- 294.6/294.6 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading matplotlib-3.8.2-cp310-cp310-win_amd64.whl (7.6 MB)\n",
      "   ---------------------------------------- 7.6/7.6 MB 3.1 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.2.0-cp310-cp310-win_amd64.whl (186 kB)\n",
      "   --------------------------------------- 186.7/186.7 kB 11.0 MB/s eta 0:00:00\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.46.0-cp310-cp310-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 2.2/2.2 MB 5.8 MB/s eta 0:00:00\n",
      "Downloading kiwisolver-1.4.5-cp310-cp310-win_amd64.whl (56 kB)\n",
      "   ---------------------------------------- 56.1/56.1 kB 3.1 MB/s eta 0:00:00\n",
      "Downloading Pillow-10.1.0-cp310-cp310-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 2.6/2.6 MB 5.5 MB/s eta 0:00:00\n",
      "Installing collected packages: pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib, seaborn\n",
      "Successfully installed contourpy-1.2.0 cycler-0.12.1 fonttools-4.46.0 kiwisolver-1.4.5 matplotlib-3.8.2 pillow-10.1.0 seaborn-0.13.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ad9deacd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='final', ylabel='count'>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x = data['final'],label = 'Sentiment Analysis of Tweets',color = 'green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ade1eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584ad87e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182641d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca3db49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dc6d99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bfa44ce",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "239c463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "39fc500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertModel,BertTokenizer,AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3a22f42b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Time of Tweet</th>\n",
       "      <th>Age of User</th>\n",
       "      <th>Country</th>\n",
       "      <th>Population -2020</th>\n",
       "      <th>Land Area (Km²)</th>\n",
       "      <th>Density (P/Km²)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>morning</td>\n",
       "      <td>0-20</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>38928346</td>\n",
       "      <td>652860.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "      <td>noon</td>\n",
       "      <td>21-30</td>\n",
       "      <td>Albania</td>\n",
       "      <td>2877797</td>\n",
       "      <td>27400.0</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>43851044</td>\n",
       "      <td>2381740.0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>morning</td>\n",
       "      <td>46-60</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>77265</td>\n",
       "      <td>470.0</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on the releases we already bought</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Angola</td>\n",
       "      <td>32866272</td>\n",
       "      <td>1246700.0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  \\\n",
       "0  cb774db0d1   \n",
       "1  549e992a42   \n",
       "2  088c60f138   \n",
       "3  9642c003ef   \n",
       "4  358bd9e861   \n",
       "\n",
       "                                                                          text  \\\n",
       "0   I`d have responded, if I were going                                          \n",
       "1   Sooo SAD I will miss you here in San Diego!!!                                \n",
       "2  my boss is bullying me...                                                     \n",
       "3   what interview! leave me alone                                               \n",
       "4   Sons of ****, why couldn`t they put them on the releases we already bought   \n",
       "\n",
       "                         selected_text sentiment Time of Tweet Age of User  \\\n",
       "0  I`d have responded, if I were going  neutral   morning       0-20         \n",
       "1  Sooo SAD                             negative  noon          21-30        \n",
       "2  bullying me                          negative  night         31-45        \n",
       "3  leave me alone                       negative  morning       46-60        \n",
       "4  Sons of ****,                        negative  noon          60-70        \n",
       "\n",
       "       Country  Population -2020  Land Area (Km²)  Density (P/Km²)  \n",
       "0  Afghanistan  38928346          652860.0         60               \n",
       "1  Albania      2877797           27400.0          105              \n",
       "2  Algeria      43851044          2381740.0        18               \n",
       "3  Andorra      77265             470.0            164              \n",
       "4  Angola       32866272          1246700.0        26               "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"train.csv\",encoding='unicode_escape')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2159e38a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on the releases we already bought</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  \\\n",
       "0  cb774db0d1   \n",
       "1  549e992a42   \n",
       "2  088c60f138   \n",
       "3  9642c003ef   \n",
       "4  358bd9e861   \n",
       "\n",
       "                                                                          text  \n",
       "0   I`d have responded, if I were going                                         \n",
       "1   Sooo SAD I will miss you here in San Diego!!!                               \n",
       "2  my boss is bullying me...                                                    \n",
       "3   what interview! leave me alone                                              \n",
       "4   Sons of ****, why couldn`t they put them on the releases we already bought  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['textID','text']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90030013",
   "metadata": {},
   "source": [
    "##\n",
    "Machine Learning models don’t work with raw text. You need to convert text to numerical representation. BERT requires even more attention when it comes to this representation.\n",
    "\n",
    "Here are the requirements:\n",
    "\n",
    "Add special tokens to separate sentences and do classification\n",
    "Pass sequences of constant length (introduce padding)\n",
    "Create array of 0s (pad token) and 1s (real token) called attention mask\n",
    "BERT offers a few model architectures and I will be using one of them combined with manual preprocessing. I am using the cased version which considers GREAT and great to be to different entities and BAD might be given more focus than bad.\n",
    "\n",
    "The tokenizer will break the sentence into words and give numerical values to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "37bc1104",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b924c645",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'https://huggingface.co/bert-based-senti'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "3fca6cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a bert based tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "bf9c4b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhavya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1929: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Bhavya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\hub.py:571: FutureWarning: Using `from_pretrained` with the url of a file (here https://huggingface.co/bert-based-senti) is deprecated and won't be possible anymore in v5 of Transformers. You should host your file on the Hub (hf.co) instead and use the repository ID. Note that this is not compatible with the caching system (your file will be downloaded at each execution) or multiple processes (each process will download the file in a different temporary file).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "404 Client Error: Not Found for url: https://huggingface.co/bert-based-senti (Request ID: Root=1-658084f4-2619e3e371aa4ca313dc5e03;66554af6-29fb-412a-8992-4e0646e9585b)\n\nSorry, we can't find the page you are looking for.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:270\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 270\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/bert-based-senti",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [183]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m access_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_hbJgElJwoxxDItImfaMknwPNNRauODDDda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mBertTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccess_token\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1985\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1983\u001b[0m         resolved_vocab_files[file_id] \u001b[38;5;241m=\u001b[39m file_path\n\u001b[0;32m   1984\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m is_remote_url(file_path):\n\u001b[1;32m-> 1985\u001b[0m         resolved_vocab_files[file_id] \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1986\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1987\u001b[0m     resolved_vocab_files[file_id] \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[0;32m   1988\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m   1989\u001b[0m         file_path,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2001\u001b[0m         _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m   2002\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\hub.py:580\u001b[0m, in \u001b[0;36mdownload_url\u001b[1;34m(url, proxies)\u001b[0m\n\u001b[0;32m    578\u001b[0m tmp_fd, tmp_file \u001b[38;5;241m=\u001b[39m tempfile\u001b[38;5;241m.\u001b[39mmkstemp()\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m os\u001b[38;5;241m.\u001b[39mfdopen(tmp_fd, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 580\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tmp_file\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:468\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, expected_size, _nb_retries)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resume_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    466\u001b[0m     headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRange\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbytes=\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (resume_size,)\n\u001b[1;32m--> 468\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHF_HUB_DOWNLOAD_TIMEOUT\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    471\u001b[0m hf_raise_for_status(r)\n\u001b[0;32m    472\u001b[0m content_length \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:426\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m    425\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m--> 426\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:330\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadRequestError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[1;32m--> 330\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/bert-based-senti (Request ID: Root=1-658084f4-2619e3e371aa4ca313dc5e03;66554af6-29fb-412a-8992-4e0646e9585b)\n\nSorry, we can't find the page you are looking for."
     ]
    }
   ],
   "source": [
    "access_token = \"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\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME, token=access_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dccdca",
   "metadata": {},
   "source": [
    "# Reference link for bert based senti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1068bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.kaggle.com/code/prakharrathi25/sentiment-analysis-using-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "9a386cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "8e83dc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'bhaavvya/bert-based-senti'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "5b60dc63",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "bhaavvya/bert-based-senti is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:270\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 270\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/bhaavvya/bert-based-senti/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\hub.py:389\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 389\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:1374\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[0;32m   1372\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[0;32m   1373\u001b[0m     \u001b[38;5;66;03m# Repo not found => let's raise the actual error\u001b[39;00m\n\u001b[1;32m-> 1374\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1376\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:1247\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1247\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[0;32m   1254\u001b[0m     \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:1624\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[0;32m   1623\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[1;32m-> 1624\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1626\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1630\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1631\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1632\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1633\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:402\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[1;32m--> 402\u001b[0m     response \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[0;32m    403\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    404\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    405\u001b[0m         follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    407\u001b[0m     )\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:426\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    425\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m--> 426\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:320\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    312\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    314\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    319\u001b[0m     )\n\u001b[1;32m--> 320\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-65808484-62704d2517d5681b5fac1313;66cafa59-cc50-4c81-9b50-37d11083fa9a)\n\nRepository Not Found for url: https://huggingface.co/bhaavvya/bert-based-senti/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[1;32mIn [180]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sentiment_analysis \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentiment-anaysis\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\__init__.py:747\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    743\u001b[0m     pretrained_model_name_or_path \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig) \u001b[38;5;129;01mand\u001b[39;00m pretrained_model_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    746\u001b[0m     \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[1;32m--> 747\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[0;32m    748\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m    749\u001b[0m         CONFIG_NAME,\n\u001b[0;32m    750\u001b[0m         _raise_exceptions_for_missing_entries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    751\u001b[0m         _raise_exceptions_for_connection_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    752\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[0;32m    753\u001b[0m     )\n\u001b[0;32m    754\u001b[0m     hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\hub.py:410\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    405\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to request access at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and pass a token having permission to this repo either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    407\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    408\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 410\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    411\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    412\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    413\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    414\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    415\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    418\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    419\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    420\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    421\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: bhaavvya/bert-based-senti is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "sentiment_analysis = pipeline('sentiment-anaysis',model = MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354ff06b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
